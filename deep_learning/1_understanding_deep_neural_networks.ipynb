{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a fundamental image classification problem where we are supposed to train a supervised machine learning model to identify if an image has a dog in it. We will have several labeled data for training. If a dog is present in the image, it has label one, and if there is no dog, the image has label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/1.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our target variable, y, will have two options 0 and 1. Now consider each image is 64 by 64 pixels in size. Since \"Red,\" \"Green,\" and \"Blue\" are the primary colors, and with their combination, we can generate other colors, we will have 64 by 64 by 3-pixel information. We will include all this information in a vector x. Say, we have \"m\" such images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/2.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Linear vs. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification problems don't work well with the linear regression algorithm since the training data is categorical. Besides, values predicted by linear regression may be outside the bounds of possible values. A logistic regression, on the other hand, works much better with a binomial classification problem. It predicts probability, and thus values are from zero to one, inclusive on both ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/4.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Approach: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the model training process, we want a model f(x) that maps an x not used for training to an appropriate y_hat. When using logistic regression, the function is σ(W.T+b). As in linear regression, we need to determine the appropriate slope and intercept values for our model; in the logistic regression, we need w.T and b. The sigmoid function, σ(x), (s-curve) is already defined as 1/(1+e^-x). Understand that w needs to be a vector of the same dimension as x. When we multiply w.T and x, we get a real number, and then we add b to the number. Finally, we calculate the sigmoid of the resulting number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/3.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to determine w.T and b in such a way that for new images, our predicted value, y_hat, and actual value, y, are the same for most of the images. To do that, we will need a cost function whose value we need to minimize. J(w,b) is the average of costs obtained using cost function L(y_hat, y) for \"m\" data points. The cost function for one data point, L, can be several functions involving y_hat and y. We can define our cost functions as well. It can be as simple as |y_hat-y|. But for machine learning algorithms, we use a cost function for which average cost, when plotted against estimated parameters (here, w.T and b), has a global minimum. For logistic regression, we use binary cross-entropy as a cost function(shown below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/5.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said earlier, we use a cost function for which average cost, when plotted against estimated parameters, has a global minimum (bowl shape). The idea is that we want to estimate the parameters w.T and b such that the average cost is minimum. Vector w has a very high dimension, and we cannot plot that many dimensions. For simplicity, we will assume it is one dimensional and ignore b. This will result in a simple two dimensional graph between J(w) and w. Notice the convex curve below obtained by plotting w against average cost function.\n",
    "\n",
    "Now, to find the maxima/minima, the mathematical procedure would be to calculate the partial derivative of J(w) with respect to w and equate it to zero. However, since w is a high dimensional vector, it gets difficult to solve it analytically. Instead of trying to calculate the exact value of w, we can estimate it. It may not be the exact value, but good enough for the task at hand with less computational expenses. The algorithm used for estimating the value of parameters to minimize average cost is known as Gradient Descent Algorithm. It is also known as the Hill climbing algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/6.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum point's left point has a negative gradient, and the minimum point's left point has a positive gradient. We start with random value of w. Using that value, we calculate J(w). Next, we calculate the first derivative at that point. If the slope is negative, we need a lower w; if it is positive, we need a higher value for w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/7.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that w is not increased/decreased randomly. It is increased or decreased by α times the first derivative of J at the previous point. The value α is known as the learning rate, and its value depends upon the work at hand. We use hyperparameter tuning along with domain knowledge for finding appropriate α. We repeat the complete cycle until convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/8.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that α can neither be too high or too low. If α is too high, we may keep missing the minimum point; if α is too low, it may take much time for convergence. Either low or high values of α for a high dimensional vector w leads to high computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/9.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector w is called the weight vector, and the number b as the bias. We add b to make our function more flexible. Note that, the cost function depends on both w and b, J(w, b) and we have only ignored b to obtain a two dimensional graph. We use the estimated w and b to calculate y_hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Summary: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us summarize the steps we followed :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/10.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computational Graphs and Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Introduction to Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computational graph is a graphical representation of steps followed to calculate the value of a function. We will explore this idea using an example.\n",
    "\n",
    "Consider a function J(a, b, c) defined as 5(ab+c).\n",
    "\n",
    "- The first step will be to calculate the product of a and b. Let's call it u.\n",
    "- The next step will be to calculate the sum of u and c. We will call it v.\n",
    "- Finally, we will multiply v with 5. We will call it J.\n",
    "\n",
    "Notice the computational graph drawn below. We have to move from left to right on the graph to calculate the final value J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/11.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Chain rule with Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have to calculate the partial derivative of J(a,b,c) with respect to a, b and c. We are familiar with the procedure using the chain rule. The chain rule for partial derivative of J with respect to a is shown below.\n",
    "\n",
    "We will try to link partial derivative with the computational graph. The idea for calculating the partial derivative with the computational graph is to move from right to left and calculate the partial derivative at each step of the variable calculated at the previous stage with respect to the variable calculated at the current phase. Finally, multiply all those partial derivatives.\n",
    "\n",
    "The steps for calculating partial derivative of J with respect to a are as follows:\n",
    "\n",
    "- First, calculate the partial derivative of J with respect to v\n",
    "- Second, calculate the partial derivative of v with respect to u\n",
    "- Next, calculate the partial derivative of u with respect to a\n",
    "- Finally, we multiply all three results to find the partial derivative of J with respect to a\n",
    "\n",
    "The process for calculating the partial derivative of J with respect to b and c is shown in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/12.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Logistic Regression with Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now form a computational graph with steps from logistic regression problem we solved above. To simplify the problem assume we have only one image vector x, i.e. the number of images, m, equals 1. Also, instead of being a large dimensional vector imagine, x is only two dimensional. Consequently, w will also be only two dimensional. Since we have only one data point, average cost function, J, will be the same as the cost function for one data point, L.\n",
    "\n",
    "Our aim with logistic regression was to determine such values for w.T and b such that the average cost is minimum. In order to do that we followed the following steps:\n",
    "\n",
    "- We started by assuming the value of w and b\n",
    "- Second, we calculated the dot product of w.T and x then sum the product with b. For a two-dimensional vector x, the result will be w1 x1 + w2 x2 + b. We will call it z\n",
    "- Next, we calculated the sigmoid of z as our predicted probability, y_hat. We will call it a for now\n",
    "- Then, we calculated the average loss function, J. For a single image data J is same as L\n",
    "\n",
    "The computational graph for the above steps is shown below. We need to move from left to right. This step, in neural networks, is known as forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/13.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to use Gradient Descent algorithm to update the values of w and b. In order to do that, we have to calculate partial derivative of J(w, b) with respect to w and b. For a single image data J is same as L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/14.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate partial derivative of *L* with respect to *w* and *b* we have to move from right to left across the computational graph.\n",
    "\n",
    "- We begin by calculating partial derivative of *L* with respect to a\n",
    "- Next, we calculate partial derivative of *a* with respect to z\n",
    "- Then, we calculate partial derivative of *z* with respect to w1, w2 and b\n",
    "\n",
    "The steps are shown below. This step, in neural networks, is known as backward propagation or backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/15.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Summary: Training w and b with one example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us summarize the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/16.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Learning from m training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not learn from a single example. We train our algorithm on several examples. So we will scale our computational graph to include m training examples.\n",
    "\n",
    "In the graph below, each x represents one image data. Sigmoid function and consequently, cost function, L, is calculated for each image, and finally, the average cost is calculated. We have to perform the backpropagation calculation for each branch. The process is repeated several times. Understand that, m can be very large, even more significant than a million, and all these computations can cost a lot. So, it is a big problem.\n",
    "\n",
    "Notice that w.T and b are constants across each branch, and the branches are not connected until we reach the calculation of J. So, a lot of parallel calculations are involved. We can take advantage of parallel computations and make the process way more efficient using the idea of Vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/17.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a matrix, X, and stack all the values of x, column-wise in it. We now have a matrix of size 12288 by m. Conventionally, a vector is represented with small letters and a two-dimensional matrix with a capital letter.   Finally, we add b to the product. The result is a vector z.\n",
    "\n",
    "Using Vectorization, we can replace several parallel calculations with one dot product. In python, it can be done with a single line of code. We can take the sigmoid of z and then compute the average cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/18.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to do the same thing in python without Vectorization, it will require a lot more calculation, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/19.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation is followed by backward propagation, and the cycle is repeated several times. The code for the process using Vectorization is shown below. The cycle has been repeated for 1000 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/20.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Logistic Regression - Implementing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have calculated appropriate w and b, we have our model. We can now take new input x, calculate z, followed by the sigmoid function. Generally, if the sigmoid function returns a value greater than or equal to 0.5, it is termed as one, and if the sigmoid function returns a value lower than 0.5, it is termed as zero. If the image contains a dog, i.e., one or not, i.e., zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/21.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above computational graph has a linear and a non-linear or activation(a) part. This combination of the linear and non-linear process is called one neuron. The decision boundary created by one neuron is linear and often not sufficient to map out complex decision boundaries, especially for higher dimensions. Thus, the need for deep neural networks arises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/22.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Problem with one neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the output decision boundary given by a neuron for two-dimensional data points. The cycle was repeated 221 times with a learning rate of 0.03 and sigmoid as the activation function. Note that both test and training loss is zero. One neuron was capable of finding a reasonable decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/23.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the output decision boundary given by a neuron for another set of two-dimensional data points. The cycle was repeated 202 times with a learning rate of 0.03 and sigmoid as the activation function. Note that both test and training loss are above 0.5. One neuron was not capable of finding a reasonable decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/24.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Neural Network Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a collection of neurons. Instead of representing a neuron as a combination of the linear and non-linear parts in neural networks, it is represented with circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/25.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows a two-layered neural network. In the first layer (hidden layer), we have four neurons, and in the second layer (output layer), we have a single neuron. We also have a layer 0/input layer representing the inputs. All layer(s) except input and output layers are referred to as hidden layer(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/26.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us revise the steps for a single neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/27.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps are repeated for deep neural networks, but we have many neurons instead of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the input and hidden layer from the above neural network. To simplify the problem, we will again assume we have only one image vector x, i.e. number of images, m, equals 1. Also, instead of being a sizeable dimensional vector imagine, x is only three dimensional. Consequently, w will also be only three dimensional. Since we have only one data point, the average cost function, J, will be the same as the cost function for one data point, L.\n",
    "\n",
    "Notice that instead of passing x to a single neuron, we are passing to four neurons. The idea is to pass the data vector to different neurons with different weight vectors and biases. A common way of restating the idea is to say each line in a neural network has a different weight. The output from the first hidden layer acts as input for the next hidden layer or output layer, and so the lines coming out of the first layer are also weighted.\n",
    "\n",
    "Suppose we consider only the first neuron from the hidden layer. Since the weight vector and bias for different neurons are different, we represent it with w1 and b1. Consequently, z becomes z1 and a becomes a1. We will have our unit neuron with linear(z1) and non-linear(a1) part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/28.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since we have multiple layers, here one hidden layer and output layer, we add [1] in the superscript to denote layer 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/29.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layer one, we have four weight vectors and biases, one for each neuron. Note that we pass the same image to all four neurons, so x vector is the same for each neuron. It results in parallel computation. To make the computations more efficient, we will use Vectorization.\n",
    "\n",
    "We will stack the transpose of weight vectors row-wise such that it represents a matrix of size four by three, represented by W. [1] in superscript represents layer 1. Similarly, z and b represent vectors after stacking row-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/30.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us talk a bit about the dimension of each term above. For this particular example, the dimension of z is (4,1), W is (4,3), x is (3,1), and b is (4,1).\n",
    "\n",
    "To generalize the dimensions, say the dimension of our input layer is represented by n with superscript [0]. The number of neurons in the first layer is represented by n with superscript [1]. The respective dimensions of z, W, x, and b are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/31.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use Vectorization, instead of calculating individuals' sigmoid, we can take the sigmoid of the vector z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/32.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every layer has a different W, and b, separated by the notation, is superscript. The output from the first layer is passed to the second layer. Except for the first layer, we can generalize the equation as shown below for all other layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/33.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Forward Propagation - m training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To repeat, we do not learn from a single example. We train our algorithm on several examples. So we will scale our computational graph to m training examples.\n",
    "\n",
    "So what we did for one image data, passing the vector components of image data to four neurons, calculating z[1] and a[1], sending a[1] as input to the output layer, and then calculating z[2] and a[2], will be represented by a single green box. The input for the box will be an image vector, and the output will be a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/34.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For m data points, we will have m such boxes. We have to calculate the cost for each image data and then calculate the average cost, J. We, again, have parallel computations, and the amount of computation has exponentially increased; therefore, we need Vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/35.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Vectorizing Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by stacking image data vectors row-wise. The size of the matrix is three by m. We already have a matrix W[1]. Note that W[1] is the same for all image vectors. W[1]X plus bias matrix will result in a matrix Z[1] of size four by m, where each column represents the result of linear computation for one image data. We apply an activation function to the matrix. Finally, we will have a matrix A[1], where each column represents a non-linear computation result for one image data.\n",
    "\n",
    "In the slide below, superscript [1] denotes layer one, and superscript (1) represents first image data. Computations are shown below for only one image vector. Also, W[1]X does not represent the actual shape of the matrix. We have used it to show that W[1] is the same for each image data, and we find dot products accordingly. The shape of W[1]X is the same as Z[1].\n",
    "\n",
    "The A[1] matrix acts as the input matrix for the output layer. Each column of A[1] represents outputs obtained from neurons in the preceding layer for one image, similar to how each column in X represents pixel values for one image. The computation process for the output layer is the same as the process for a neuron with m data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/36.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first graph below shows the forward propagation computational graph before Vectorization of m data points. The second graph is a simplified computational graph for forward propagation of a two layered neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/37.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Forward Propagation in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the python code for a two-layered neural network is shown below. The first set of code is used if we do not use vectorization techniques; the second set of code is used if we use vectorization.\n",
    "\n",
    "Note that the second set is more comfortable to write and much better computationally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/38.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, in this blog, we have used the sigmoid function as our activation function. Nevertheless, does it always need to be sigmoid? No. For neural networks, we have many other non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/39.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the standard activation functions are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/40.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations of standard activation functions and their derivatives are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/52.png\" width=600 align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that activation functions can vary with each layer. Activation functions, in general, are represented by g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/41.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. Backpropagation - Revisiting Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Backward propagation, for logistic regression, we calculated the partial derivative of average cost with respect to weight and bias and used it to update values of weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/42.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. Backpropagation for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of neural networks, we do not have just w and b; instead, we have W[1], b[1], and W[2], b[2]. So we need to compute the partial derivative of average cost, J, with respect to all four parameters. However, the process is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/43.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we calculate J's partial derivative with respect to A[2] and then with respect to Z[2]. Since W[2] and b[2] are associated with Z[2], we have to calculate the partial derivative of Z[2] with respect to W[2] and b[2] before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/44.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the first layer, we need to compute the partial derivative of Z[2] with respect to A[1]. Then we calculate partial derivative A[1] with respect to Z[1]. Since W[1] and b[1] are associated with Z[1], we have to calculate the partial derivative of Z[1] with respect to W[1] and b[1].\n",
    "\n",
    "Finally, we update W[1], b[1] and W[2], b[2]. We repeat the cycle until a sufficiently low average cost is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/45.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10. Summary of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us summarize the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/46.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our optimum W[1], b[1], and W[2], b[2], we can begin predicting. We input a new x and pass it through layers 1 and 2 to get our final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/47.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12. Initializing of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us talk a little about initializing W[1], b[1], and W[2], b[2]. Can we initialize it with any numbers?\n",
    "\n",
    "Say, our image vector is only two dimensional, and we have to pass it through two neurons only. Consequently, W[1] will have a dimension (2, 2). We initialize W[1] as a null matrix and b[1] as a null vector. In such a case, irrespective of the value of x, z[1] will be a null vector. If we choose sigmoid as activation function, a[1] will be a vector will all elements as 0.5.\n",
    "\n",
    "During backpropagation, we need to calculate partial derivatives. Those values will also be symmetric. No matter how many iterations we try, we will not break symmetry, i.e., all vector or matrix elements will always be equal. In most cases, it will not result in the minimum value of the average cost. Generally, we randomly initialize W and make sure that it is not a null matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/48.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we are trying to learn an f(x) for a machine learning problem that can take new input and make appropriate predictions. Logistic regression is one of the methods used for the task, but it is a relatively simple model and cannot form complex decision boundaries. Thus, we need neural networks.\n",
    "\n",
    "If we decide to use sigmoid as our activation function, a unit neuron is conceptually the same as logistic regression. In order to generate complex decision boundaries, we use several neurons arranged in several layers. In essence, we will be performing the same calculations we did with logistic regression, but a lot many times.\n",
    "\n",
    "Mathematical equations for logistic regression and neural networks are shown below, assuming we decide to use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Understanding%20Deep%20Neural%20Networks/49.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you liked this blog. I know it was long, but I wanted to start at the beginning. Note that I have taken slides from Dr. Remun Koirala's presentation on Deep Neural Networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
